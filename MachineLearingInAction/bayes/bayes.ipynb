{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1、创建实验样本，postingList返回进行词条切分后的文档集合，classVec是文档集合中各个文档所对应的类别标签的集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my','dog','has','flea','problems','help','please'],\n",
    "                ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "                ['my','dalmation','is','so','cute','I','love','him'],\n",
    "                ['stop','posting','stupid','worthless','garbage'],\n",
    "                ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "                ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec=[0,1,0,1,0,1] # 1代表侮辱性文字，0代表正常言论\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2、创建词汇表（在所有文档中出现的不重复词的列表）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet): # dataSet为一个文档集合\n",
    "    vocabSet=set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet=vocabSet | set(document)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3、根据词汇表和输入的文档，输出对应的文档向量--词集模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList,inputSet): # vocabList为词汇表，inputSet为输入的某个文档\n",
    "    returnVec=[0]*len(vocabList) # returnVec为文档向量\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]=1\n",
    "        else:\n",
    "            print (\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据词汇表和输入的文档，输出对应的文档向量--词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2Vec(vocabList,inputSet): # vocabList为词汇表，inputSet为输入的某个文档\n",
    "    returnVec=[0]*len(vocabList) # returnVec为文档向量\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] +=1\n",
    "        else:\n",
    "            print (\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 测试createVocabList方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#listOPosts,listClasses=loadDataSet()\n",
    "#myVocabList=createVocabList(listOPosts)\n",
    "#myVocabList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 测试setOfWords2Vec方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#setOfWords2Vec(myVocabList,listOPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs=len(trainMatrix) # 获取训练样本的文档数：6\n",
    "    numWords=len(trainMatrix[0]) # 获取单个文档中的单词个数：32\n",
    "    print (numTrainDocs,numWords,trainCategory,numTrainDocs)\n",
    "    pAbusive=sum(trainCategory)/float(numTrainDocs)  # 训练样本标签sum/训练样本文档总数：3/6\n",
    "    print (sum(trainCategory))\n",
    "    print (pAbusive)\n",
    "    \n",
    "    # 避免因多个概率相乘中有值为0产生最终为0的结果不准确的情况，将所有词的初始出现次数置为1，并将分母初始化为2\n",
    "    p0Num=np.ones(numWords) # 单个文档的标签零矩阵：正常词汇\n",
    "    p1Num=np.ones(numWords) # 单个文档的标签零矩阵：侮辱性词汇\n",
    "    p0Denom=2.0;p1Denom=2.0 \n",
    "    for i in range(numTrainDocs): # 对每个文档执行以下操作\n",
    "        if trainCategory[i]==1: # 如果是侮辱性文字,执行以下操作\n",
    "            p1Num+=trainMatrix[i] # \n",
    "            p1Denom+=sum(trainMatrix[i]) # sum(trainMatrix[0])为8\n",
    "        else:\n",
    "            p0Num+=trainMatrix[i]\n",
    "            p0Denom+=sum(trainMatrix[i])\n",
    "    p1Vect=log(p1Num/p1Denom) # 避免多个小数相乘产生的下溢出，对原值取对数\n",
    "    print (p1Num,p1Denom,p1Vect)\n",
    "    p0Vect=log(p0Num/p0Denom) # 避免多个小数相乘产生的下溢出，对原值取对数\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 32 [0, 1, 0, 1, 0, 1] 6\n",
      "3\n",
      "0.5\n",
      "[0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 3. 0. 1. 2. 0. 1. 2. 0. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1.] 19.0 [0.         0.         0.         0.         0.05263158 0.05263158\n",
      " 0.         0.05263158 0.         0.05263158 0.05263158 0.\n",
      " 0.05263158 0.15789474 0.         0.05263158 0.10526316 0.\n",
      " 0.05263158 0.10526316 0.         0.05263158 0.         0.05263158\n",
      " 0.         0.         0.         0.05263158 0.         0.\n",
      " 0.         0.05263158]\n",
      "[0.04166667 0.04166667 0.04166667 0.04166667 0.         0.04166667\n",
      " 0.04166667 0.         0.04166667 0.         0.         0.04166667\n",
      " 0.         0.         0.04166667 0.         0.         0.04166667\n",
      " 0.         0.04166667 0.04166667 0.04166667 0.04166667 0.\n",
      " 0.04166667 0.125      0.04166667 0.08333333 0.04166667 0.04166667\n",
      " 0.04166667 0.        ] [0.         0.         0.         0.         0.05263158 0.05263158\n",
      " 0.         0.05263158 0.         0.05263158 0.05263158 0.\n",
      " 0.05263158 0.15789474 0.         0.05263158 0.10526316 0.\n",
      " 0.05263158 0.10526316 0.         0.05263158 0.         0.05263158\n",
      " 0.         0.         0.         0.05263158 0.         0.\n",
      " 0.         0.05263158] 0.5\n"
     ]
    }
   ],
   "source": [
    "listOpsts,listClasses=loadDataSet()\n",
    "myVocabList=createVocabList(listOpsts)\n",
    "trainMat=[] \n",
    "#listOpsts 文档矩阵\n",
    "#listClasses 标签集合\n",
    "#myVocabList 唯一性的词条集合\n",
    "# trainMat 每个文档中的所有词条对于的标签向量矩阵\n",
    "for postinDoc in listOpsts: # 对于文档矩阵中的每一个文档，进行以下操作\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc)) # 将每个文档对于的向量append到trainMat中\n",
    "p0V,p1V,pAb=trainNB0(trainMat,listClasses)\n",
    "print (p0V,p1V,pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建完整分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    p1=sum(vec2Classify * p1Vec) + math.log(pClass1)\n",
    "    p0=sum(vec2Classify * p0Vec) + math.log(1.0-pClass1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPosts,listClasses=loadDataSet()\n",
    "    myVocabList=createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOpsts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "    p0v,p1v,pAb=trainNB0(np.array(trainMat),np.array(listClasses))\n",
    "    testEntry=['love','my','dalmation']\n",
    "    thisDoc=np.array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print (testEntry,'classified as: ',classifyNB(thisDoc,p0v,p1v,pAb))\n",
    "    testEntry=['stupid','garbage']\n",
    "    thisDoc=np.array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,'classified as: ',classifyNB(thisDoc,p0v,p1v,pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 32 [0 1 0 1 0 1] 6\n",
      "3\n",
      "0.5\n",
      "[0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 3. 0. 1. 2. 0. 1. 2. 0. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1.] 19.0 [0.         0.         0.         0.         0.05263158 0.05263158\n",
      " 0.         0.05263158 0.         0.05263158 0.05263158 0.\n",
      " 0.05263158 0.15789474 0.         0.05263158 0.10526316 0.\n",
      " 0.05263158 0.10526316 0.         0.05263158 0.         0.05263158\n",
      " 0.         0.         0.         0.05263158 0.         0.\n",
      " 0.         0.05263158]\n",
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
